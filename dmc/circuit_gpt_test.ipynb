{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf895e56-dbe6-4871-be60-93aab1e3f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pickle\n",
    "import transformer_lens\n",
    "from torch.optim import AdamW\n",
    "from os.path import join\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from circuit_gpt import CircuitGPT, CircuitGPTConfig\n",
    "from ioi_dataset import IOIDataset, CircuitIOIDataset, prepare_batch_inputs, prepare_ioi_data_for_clm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06764a05-18d7-47dc-81e2-621c83000297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fccbb119-a88e-432c-b8a3-e180d2b6d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiffMaskArgs:\n",
    "    model_dir: str = '/home/leiyu/projects/def-yangxu/leiyu/LMs/'\n",
    "    data_dir: str = '/home/leiyu/projects/def-yangxu/leiyu/circuit-discovery/data/'\n",
    "    results_dir: str = '/home/leiyu/scratch/circuit-discovery/mask_logits/'\n",
    "    model_name: str = 'gpt2-small'\n",
    "    gs_temp_weight: float = 0.01\n",
    "    gs_temp_edge: float = 1.0\n",
    "    logits_w_init: float = 0.0\n",
    "    logits_e_init: float = 0.0\n",
    "    batch_size: int = 32\n",
    "    train_epochs_weight: int = 1000\n",
    "    train_epochs_edge: int = 50\n",
    "    lr_weight: float = 0.1\n",
    "    lr_edge: float = 0.1\n",
    "    lambda_sparse_weight_init: float = 1.\n",
    "    lambda_sparse_edge_init: float = 1.\n",
    "    lambda_complete_weight_init: float = 1.\n",
    "    lambda_complete_edge_init: float = 1.\n",
    "    save_every: int = 5\n",
    "    resume_epoch_w: int = 0\n",
    "    resume_epoch_e: int = 0\n",
    "    use_weight_masks: bool = False\n",
    "    n_epoch_warmup_lambda_sparse: int = 20\n",
    "    n_epoch_cooldown_lambda_sparse: int = 20\n",
    "    max_times_lambda_sparse: float = 100.\n",
    "    min_times_lambda_sparse: float = 0.01\n",
    "    n_ioi_data: int = 640\n",
    "\n",
    "\n",
    "def compute_faith_loss(batch_logits, batch_inputs):\n",
    "    # batch_logits: (B, seq_len, vocab_size)\n",
    "    batch_seq_lens = batch_inputs['seq_lens']\n",
    "    batch_size = batch_logits.shape[0]\n",
    "\n",
    "    logits_target_good = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target good']]\n",
    "    logits_target_bad = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target bad']]\n",
    "    logits_gb = torch.stack([logits_target_good, logits_target_bad], -1)  # (B,2)\n",
    "\n",
    "    batch_labels = torch.zeros(batch_size).long().to(logits_gb.device)\n",
    "    batch_faith_loss = nn.functional.cross_entropy(logits_gb, batch_labels)\n",
    "\n",
    "    return batch_faith_loss, logits_gb\n",
    "\n",
    "\n",
    "def compute_complete_loss(batch_logits, batch_inputs):\n",
    "    # batch_logits: (B, seq_len, vocab_size)\n",
    "    batch_seq_lens = batch_inputs['seq_lens']\n",
    "    batch_size = batch_logits.shape[0]\n",
    "\n",
    "    logits_target_good = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target good']]\n",
    "    logits_target_bad = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target bad']]\n",
    "    logits_gb = torch.stack([logits_target_good, logits_target_bad], -1)  # (B,2)\n",
    "\n",
    "    batch_probs_uniform = torch.ones(logits_gb.shape).to(logits_gb.device) * 0.5\n",
    "    batch_complete_loss = nn.functional.cross_entropy(logits_gb, batch_probs_uniform)\n",
    "\n",
    "    return batch_complete_loss, logits_gb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_dl, tokenizer, device, use_weight_mask=True, use_edge_mask=True, reverse=False):\n",
    "    model.eval()\n",
    "\n",
    "    # get weight and edge density     \n",
    "    model.turn_on_weight_masks(deterministic=True, reverse=reverse)  \n",
    "    _, _, weight_density = model.get_weight_density()       \n",
    "    model.turn_on_edge_masks(deterministic=True, reverse=reverse)  \n",
    "    _, _, edge_density = model.get_edge_density()\n",
    "\n",
    "    if not use_weight_mask:\n",
    "        model.turn_off_weight_masks()\n",
    "    if not use_edge_mask:     \n",
    "        model.turn_off_edge_masks()\n",
    "\n",
    "    total = len(eval_dl.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "        batch_logits = model(batch_inputs['input_ids'].to(device))[0]  # (B, seq_len, vocab_size)\n",
    "        _, batch_logits_gb = compute_faith_loss(batch_logits, batch_inputs)\n",
    "        # print(batch_logits_gb)\n",
    "        correct += (batch_logits_gb[:, 0] > batch_logits_gb[:, 1]).sum().cpu().item()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.turn_off_weight_masks()\n",
    "    model.turn_off_edge_masks()\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    return acc, weight_density, edge_density\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9f621b-7ad3-4914-8766-6b8f9beb9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('diff_mask_ioi.yml') as f:\n",
    "#     args = yaml.safe_load(f)\n",
    "args = DiffMaskArgs()\n",
    "model_path = join(args.model_dir, args.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = IOIDataset(prompt_type=\"ABBA\", N=args.n_ioi_data, tokenizer=tokenizer)\n",
    "ds_train, ds_test = train_test_split(ds.ioi_prompts, test_size=0.2, random_state=0)\n",
    "# Note that there are overlaps between train and test sets, due to the way IOIDataset is constructed (randomly sample N items)\n",
    "\n",
    "ioi_ds_train = CircuitIOIDataset(prepare_ioi_data_for_clm(ds_train))\n",
    "ioi_ds_test = CircuitIOIDataset(prepare_ioi_data_for_clm(ds_test))\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    ioi_ds_train,\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "eval_dl = DataLoader(\n",
    "    ioi_ds_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f715754b-e93b-4ddf-8c6e-465782c13abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# # download gpt2-small weights from EasyTransformer and save it\n",
    "# reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "# torch.save(reference_gpt2.state_dict(), join(args['model_dir'], 'gpt2-small/gpt2_small_weights.pt'))\n",
    "\n",
    "gpt_weights = torch.load(join(model_path, 'model_weights.pt')) \n",
    "circuit_gpt_config = CircuitGPTConfig(\n",
    "    debug=False,\n",
    "    gs_temp_weight=args.gs_temp_weight,\n",
    "    gs_temp_edge=args.gs_temp_edge,\n",
    "    use_weight_masks=False\n",
    ")\n",
    "circuit_gpt = CircuitGPT(circuit_gpt_config)\n",
    "circuit_gpt.load_pretrained_weight(gpt_weights)\n",
    "\n",
    "# load pretrained mask logits if necessary\n",
    "if args.resume_epoch_w > 0:\n",
    "    weight_mask_logits = torch.load(join(model_path), f'weight_mask_logits_{resume_epoch_w}.pt')\n",
    "    circuit_gpt.load_pretrained_weight_mask(weight_mask_logits)\n",
    "if args.resume_epoch_e > 0:\n",
    "    edge_mask_logits = torch.load(join(model_path), f'edge_mask_logits_{resume_epoch_e}.pt')\n",
    "    circuit_gpt.load_pretrained_edge_mask(edge_mask_logits)\n",
    "\n",
    "circuit_gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c117e8-8d97-42d9-bcba-7dca63b1acb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. mean pruned model accuracy: 1.00,weight density: 1.0000,edge density: 1.0000\n"
     ]
    }
   ],
   "source": [
    "eval_acc, weight_density, edge_density = eval_model(\n",
    "    circuit_gpt, eval_dl, tokenizer, device, \n",
    "    use_weight_mask=False, use_edge_mask=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Epoch 0. mean pruned model accuracy: {eval_acc:.4f},\" + \n",
    "    f\"weight density: {weight_density:.4f},\" + \n",
    "    f\"edge density: {edge_density:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ae6d0e-2fa9-4dd7-b86a-92049a39869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_logits = [mask for _, mask in circuit_gpt.mask_logits_dict_weight.items()]\n",
    "edge_logits = [mask for _, mask in circuit_gpt.mask_logits_dict_edge.items()]\n",
    "\n",
    "# optim_weight = torch.optim.AdamW(weight_logits, lr=args.lr_weight)\n",
    "optim_edge = torch.optim.AdamW(edge_logits, lr=args.lr_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34d431-3595-416f-86fc-02268bc09d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665f4cf3-85cb-41e5-a9e7-70b536a44ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f7532449d74f53902c6302d084eaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. discovered circuit accuracy: 0.9746, complementary circuit accuracy: 0.4785, weight density: 1.0000, edge density: 0.9922\n",
      "Epoch 2. discovered circuit accuracy: 0.5312, complementary circuit accuracy: 0.4785, weight density: 1.0000, edge density: 0.8904\n",
      "Epoch 3. discovered circuit accuracy: 0.7070, complementary circuit accuracy: 0.5000, weight density: 1.0000, edge density: 0.7580\n",
      "Epoch 4. discovered circuit accuracy: 0.7129, complementary circuit accuracy: 0.5176, weight density: 1.0000, edge density: 0.6174\n",
      "Epoch 5. discovered circuit accuracy: 0.7812, complementary circuit accuracy: 0.5078, weight density: 1.0000, edge density: 0.4917\n",
      "Epoch 6. discovered circuit accuracy: 0.6270, complementary circuit accuracy: 0.5000, weight density: 1.0000, edge density: 0.3864\n",
      "Epoch 7. discovered circuit accuracy: 0.5469, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.3001\n",
      "Epoch 8. discovered circuit accuracy: 0.5684, complementary circuit accuracy: 0.4785, weight density: 1.0000, edge density: 0.2353\n",
      "Epoch 9. discovered circuit accuracy: 0.5918, complementary circuit accuracy: 0.4824, weight density: 1.0000, edge density: 0.1883\n",
      "Epoch 10. discovered circuit accuracy: 0.6230, complementary circuit accuracy: 0.4824, weight density: 1.0000, edge density: 0.1510\n",
      "Epoch 11. discovered circuit accuracy: 0.5625, complementary circuit accuracy: 0.4590, weight density: 1.0000, edge density: 0.1255\n",
      "Epoch 12. discovered circuit accuracy: 0.5781, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.1048\n",
      "Epoch 13. discovered circuit accuracy: 0.5781, complementary circuit accuracy: 0.4902, weight density: 1.0000, edge density: 0.0873\n",
      "Epoch 14. discovered circuit accuracy: 0.6543, complementary circuit accuracy: 0.4980, weight density: 1.0000, edge density: 0.0741\n",
      "Epoch 15. discovered circuit accuracy: 0.6484, complementary circuit accuracy: 0.4961, weight density: 1.0000, edge density: 0.0628\n",
      "Epoch 16. discovered circuit accuracy: 0.6582, complementary circuit accuracy: 0.4922, weight density: 1.0000, edge density: 0.0546\n",
      "Epoch 17. discovered circuit accuracy: 0.6699, complementary circuit accuracy: 0.4922, weight density: 1.0000, edge density: 0.0493\n",
      "Epoch 18. discovered circuit accuracy: 0.8516, complementary circuit accuracy: 0.4941, weight density: 1.0000, edge density: 0.0441\n",
      "Epoch 19. discovered circuit accuracy: 0.6973, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0402\n",
      "Epoch 20. discovered circuit accuracy: 0.7930, complementary circuit accuracy: 0.4922, weight density: 1.0000, edge density: 0.0370\n",
      "Epoch 21. discovered circuit accuracy: 0.7324, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0350\n",
      "Epoch 22. discovered circuit accuracy: 0.8281, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0340\n",
      "Epoch 23. discovered circuit accuracy: 0.7988, complementary circuit accuracy: 0.4941, weight density: 1.0000, edge density: 0.0335\n",
      "Epoch 24. discovered circuit accuracy: 0.8691, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0332\n",
      "Epoch 25. discovered circuit accuracy: 0.8379, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0328\n",
      "Epoch 26. discovered circuit accuracy: 0.8125, complementary circuit accuracy: 0.4922, weight density: 1.0000, edge density: 0.0317\n",
      "Epoch 27. discovered circuit accuracy: 0.8379, complementary circuit accuracy: 0.4941, weight density: 1.0000, edge density: 0.0309\n",
      "Epoch 28. discovered circuit accuracy: 0.9121, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0307\n",
      "Epoch 29. discovered circuit accuracy: 0.9414, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0307\n",
      "Epoch 30. discovered circuit accuracy: 0.9121, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0308\n",
      "Epoch 31. discovered circuit accuracy: 0.8398, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0311\n",
      "Epoch 32. discovered circuit accuracy: 0.8281, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0312\n",
      "Epoch 33. discovered circuit accuracy: 0.9141, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0312\n",
      "Epoch 34. discovered circuit accuracy: 0.8105, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0315\n",
      "Epoch 35. discovered circuit accuracy: 0.9629, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0319\n",
      "Epoch 36. discovered circuit accuracy: 0.9062, complementary circuit accuracy: 0.4805, weight density: 1.0000, edge density: 0.0315\n",
      "Epoch 37. discovered circuit accuracy: 0.8496, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0320\n",
      "Epoch 38. discovered circuit accuracy: 0.9062, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0335\n",
      "Epoch 39. discovered circuit accuracy: 0.8848, complementary circuit accuracy: 0.4902, weight density: 1.0000, edge density: 0.0345\n",
      "Epoch 40. discovered circuit accuracy: 0.8711, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0351\n",
      "Epoch 41. discovered circuit accuracy: 0.8730, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0354\n",
      "Epoch 42. discovered circuit accuracy: 0.7988, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0362\n",
      "Epoch 43. discovered circuit accuracy: 0.8125, complementary circuit accuracy: 0.4805, weight density: 1.0000, edge density: 0.0371\n",
      "Epoch 44. discovered circuit accuracy: 0.9492, complementary circuit accuracy: 0.4805, weight density: 1.0000, edge density: 0.0385\n",
      "Epoch 45. discovered circuit accuracy: 0.9453, complementary circuit accuracy: 0.4824, weight density: 1.0000, edge density: 0.0391\n",
      "Epoch 46. discovered circuit accuracy: 0.9707, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0397\n",
      "Epoch 47. discovered circuit accuracy: 0.9922, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0402\n",
      "Epoch 48. discovered circuit accuracy: 0.9766, complementary circuit accuracy: 0.4922, weight density: 1.0000, edge density: 0.0411\n",
      "Epoch 49. discovered circuit accuracy: 0.9668, complementary circuit accuracy: 0.4863, weight density: 1.0000, edge density: 0.0423\n",
      "Epoch 50. discovered circuit accuracy: 0.9648, complementary circuit accuracy: 0.4883, weight density: 1.0000, edge density: 0.0435\n",
      "Epoch 51. discovered circuit accuracy: 0.9453, complementary circuit accuracy: 0.4844, weight density: 1.0000, edge density: 0.0440\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m eval_acc_pruned, weight_density, edge_density \u001b[38;5;241m=\u001b[39m eval_model(\n\u001b[1;32m     45\u001b[0m     circuit_gpt, eval_dl, tokenizer, device, use_weight_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m eval_acc_complement, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcircuit_gpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_weight_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. discovered circuit accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, complementary circuit accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, weight density: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, edge density: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     52\u001b[0m         epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, eval_acc_pruned, eval_acc_complement, weight_density, edge_density)\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# save good edge masks\u001b[39;00m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, eval_dl, tokenizer, device, use_weight_mask, use_edge_mask, reverse)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m eval_dl:\n\u001b[1;32m     79\u001b[0m     batch_inputs \u001b[38;5;241m=\u001b[39m prepare_batch_inputs(batch, tokenizer)\n\u001b[0;32m---> 80\u001b[0m     batch_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     _, batch_logits_gb \u001b[38;5;241m=\u001b[39m compute_faith_loss(batch_logits, batch_inputs)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# print(batch_logits_gb)\u001b[39;00m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre06/project/6070308/leiyu/circuit-discovery/circuit_gpt.py:325\u001b[0m, in \u001b[0;36mCircuitGPT.forward\u001b[0;34m(self, tokens, return_states)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(residual, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch position d_model -> batch position 1 d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 325\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_states:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre06/project/6070308/leiyu/circuit-discovery/circuit_gpt.py:240\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre)\u001b[0m\n\u001b[1;32m    237\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(normalized_resid_mid)\n\u001b[1;32m    238\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(mlp_out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch position d_model -> batch position 1 d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_lambda_sparse(epoch, lambda_0, max_times=100., min_times=0.001, \n",
    "                      n_epoch_warmup=10, n_epoch_cooldown=10):\n",
    "\n",
    "    if epoch < n_epoch_warmup:\n",
    "        return lambda_0  + lambda_0 * (max_times - 1) * epoch / n_epoch_warmup\n",
    "        \n",
    "    elif epoch < n_epoch_warmup + n_epoch_cooldown:\n",
    "        return lambda_0 * max_times - lambda_0 * (max_times - min_times) * (epoch - n_epoch_warmup) / n_epoch_cooldown\n",
    "        \n",
    "    else:\n",
    "        return lambda_0 * min_times\n",
    "        \n",
    "# it takes about 35 mins to run 100 epochs of edge mask training on one A100 GPU with batch_size=32\n",
    "for epoch in tqdm(range(args.train_epochs_edge)):\n",
    "    lambda_sparse_edge = get_lambda_sparse(\n",
    "        epoch, \n",
    "        lambda_0=args.lambda_sparse_edge_init,\n",
    "        max_times=args.max_times_lambda_sparse, \n",
    "        min_times=args.min_times_lambda_sparse, \n",
    "        n_epoch_warmup=args.n_epoch_warmup_lambda_sparse,\n",
    "        n_epoch_cooldown=args.n_epoch_cooldown_lambda_sparse,\n",
    "    )\n",
    "    lambda_complete_edge = args.lambda_complete_edge_init\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "        \n",
    "        circuit_gpt.turn_on_edge_masks(deterministic=False)\n",
    "        sparse_loss_edge = circuit_gpt.edge_sparseness_loss()\n",
    "    \n",
    "        batch_logits = circuit_gpt(batch_inputs['input_ids'].to(device))[0] \n",
    "        faith_loss_edge, _ = compute_faith_loss(batch_logits, batch_inputs) \n",
    "        \n",
    "        circuit_gpt.turn_on_edge_masks(deterministic=False, reverse=True)\n",
    "        batch_logits = circuit_gpt(batch_inputs['input_ids'].to(device))[0] \n",
    "        complete_loss_edge, _ = compute_complete_loss(batch_logits, batch_inputs) \n",
    "        \n",
    "        loss_edge = faith_loss_edge + sparse_loss_edge *  lambda_sparse_edge + complete_loss_edge * lambda_complete_edge\n",
    "        loss_edge.backward()\n",
    "        optim_edge.step()\n",
    "        optim_edge.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    eval_acc_pruned, weight_density, edge_density = eval_model(\n",
    "        circuit_gpt, eval_dl, tokenizer, device, use_weight_mask=False, reverse=False\n",
    "    )\n",
    "    eval_acc_complement, _, _ = eval_model(\n",
    "        circuit_gpt, eval_dl, tokenizer, device, use_weight_mask=False, reverse=True\n",
    "    )\n",
    "    print(\n",
    "        \"Epoch {}. discovered circuit accuracy: {:.4f}, complementary circuit accuracy: {:.4f}, weight density: {:.4f}, edge density: {:.4f}\".format(\n",
    "            epoch + 1, eval_acc_pruned, eval_acc_complement, weight_density, edge_density)\n",
    "    )\n",
    "\n",
    "    # save good edge masks\n",
    "    if eval_acc_pruned > 0.95 and edge_density < 0.05:\n",
    "        torch.save(\n",
    "            circuit_gpt.mask_logits_dict_edge,\n",
    "            join(args.results_dir, f'mask_logits_dict_edge_ioi_edge_only_{epoch}.pt')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6f269-c500-4d31-b408-1f7b385faf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77479ca-4374-41fe-86ce-926a997c49a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddf6d8-0725-4efd-99d6-0f4509a84d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a592bc-48c1-4baa-ab3a-7081cc2d8830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b0502-70c5-4381-9edc-ca7219807216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d968469-bac0-4ef9-a95e-e579266580d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd02b4-e70f-474b-8dec-a4f28dad1e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed5943-899a-4bdb-8b35-855c2739b7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bf1bd-227d-447f-b4c3-f78dc1ec98e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418fe0fb-97d6-4d8f-807a-43bb37ddeff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfc5f5-0343-42b8-8cb1-cd1a255f4133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8a44c7-5a78-481b-81c4-2380bfa9fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028426a3-108b-4303-bdb0-19dbf3abce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[0] < x[1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7dcd43-bba1-42c3-b859-cb59fe9f777d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
