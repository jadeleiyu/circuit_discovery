{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38b3763-d84c-472f-872d-16ff35abdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pickle\n",
    "import transformer_lens\n",
    "from torch.optim import AdamW\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/leiyu/projects/def-yangxu/leiyu/circuit-discovery')\n",
    "from dmc.circuit_gpt import *\n",
    "from oqa_dataset import *\n",
    "from oqa_utils import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_dir = '/home/leiyu/projects/def-yangxu/leiyu/circuit-discovery/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fbcd9c-234c-4ebf-b4f7-f567997a6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OQA data\n",
    "ds_dict = pickle.load(open(join(data_dir, 'pararel_capital_ds_dict.p'), 'rb'))\n",
    "full_model_target_log_probs = torch.load(f'full_model_results/target_log_probs.pt')\n",
    "full_model_pred_labels = torch.load(f'full_model_results/pred_labels.pt')\n",
    "capital_vocab_idx = torch.load(f'full_model_results/capital_vocab_idx.pt')\n",
    "ds_dict['full_model_target_log_probs'] = full_model_target_log_probs\n",
    "ds_dict['full_model_pred_labels'] = full_model_pred_labels\n",
    "\n",
    "ds = OQACircuitDataset(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6220c55c-1fc3-4e4c-8d3c-9dc54881e009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d248d1-bc63-4a37-b316-65e75f17a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "for i in range(len(ds)):\n",
    "    if ds[i]['label'] == ds[i]['full_model_pred_label']:\n",
    "        n_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d2248e-38e6-49a9-ab0e-dfd1756b2350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3479188900747065"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct / 937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef41028-d4a4-44d7-b59a-ba867ba82d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The capital of Azerbaijan is',\n",
       " 'label': 433,\n",
       " 'full_model_target_log_probs': tensor([ -8.1028, -12.7846,  -9.2102, -12.3880,  -9.7646,  -7.5984, -14.1302,\n",
       "          -9.1673, -17.0951, -10.6358, -13.7977, -10.8338, -10.1600,  -7.4719,\n",
       "         -10.9165, -10.8896,  -7.0938,  -5.9943,  -8.9910,  -7.3838, -13.6729,\n",
       "         -10.7229,  -7.3618, -10.0352,  -8.7672, -10.4191, -10.9485,  -3.1239,\n",
       "          -4.2082, -11.3415, -11.2197, -11.1878,  -9.3578,  -9.2304, -11.5418,\n",
       "         -10.6972,  -8.5861, -10.8091,  -2.8443,  -6.9939,  -7.9517, -10.7942,\n",
       "         -11.2243, -12.4147, -13.5209,  -6.1308,  -8.9237,  -4.1074, -14.6083,\n",
       "          -8.3737, -11.4914,  -9.2978,  -6.1308,  -9.9293,  -9.2709,  -8.7131,\n",
       "          -4.0978, -13.1979, -13.2176,  -4.0978,  -6.4935, -10.0638,  -9.6405,\n",
       "         -10.9531,  -3.1239, -10.6628, -14.0896, -12.9372, -10.1742,  -8.9641,\n",
       "         -11.3667, -11.8434, -13.6609,  -9.6617, -11.6937, -15.0913, -10.6275,\n",
       "         -13.5518,  -9.1191,  -7.4061, -10.5028, -12.2676,  -5.1541, -11.7840,\n",
       "          -6.5733,  -4.9297,  -8.9431,  -3.8073, -12.6781,  -8.0824, -12.7106,\n",
       "          -8.7751, -11.7023,  -7.8044, -12.2610, -11.5259, -11.7177,  -4.0794,\n",
       "         -10.9077,  -9.6636, -12.1527, -12.4907, -11.3779, -10.3231, -10.9014,\n",
       "          -9.8372, -12.6339, -11.8282,  -9.0718,  -3.1239, -11.0403, -10.9582,\n",
       "          -8.9425, -10.8064, -15.3447, -13.8397,  -9.8612,  -8.6490,  -3.1239,\n",
       "         -14.2465, -11.2511,  -9.7899,  -7.8085,  -5.5431, -10.3906,  -8.4973,\n",
       "         -13.3810,  -8.7192, -11.9736,  -8.0477, -12.6235, -10.4676,  -5.7799,\n",
       "         -13.0881, -12.3749,  -6.1262, -12.8123,  -8.0024, -13.2276, -12.0066,\n",
       "         -10.9185,  -7.8794, -12.5741, -12.9795, -11.8613, -11.2996, -10.8663,\n",
       "          -7.9277, -11.0644,  -9.6906, -11.8832, -10.5383, -13.8939,  -7.1938,\n",
       "         -10.2586,  -8.0683,  -7.1105,  -9.3963,  -7.2922,  -7.9920, -12.1764,\n",
       "         -13.2124, -10.2139,  -9.9722, -12.3934,  -8.0501, -10.6685, -10.0089,\n",
       "         -12.2877, -12.5282, -11.9586,  -9.4458, -12.0239, -14.5221,  -9.4578,\n",
       "         -11.8195, -14.2296, -12.2288, -10.0204,  -9.6920, -10.7623, -10.0435,\n",
       "          -4.7628,  -8.6907,  -9.1648, -11.7950, -11.3872,  -3.8621, -10.7485,\n",
       "          -9.7317, -11.0916,  -9.9869,  -9.3315, -11.2906,  -3.6985,  -9.0452,\n",
       "         -10.1449,  -9.3427,  -7.1744, -10.5991,  -4.9652,  -6.7648, -13.7153,\n",
       "         -14.1577, -11.9858, -13.1150,  -9.4641,  -7.4318,  -9.6263,  -5.4493,\n",
       "         -10.6369,  -5.1416,  -8.2128,  -9.2670,  -8.7879,  -4.5673,  -7.8088,\n",
       "         -12.8860, -10.6870,  -7.3395, -13.3058,  -9.7399,  -4.7628,  -7.1385,\n",
       "          -9.9626, -10.8035, -11.8534,  -9.0374,  -9.7974, -10.4460,  -9.5937,\n",
       "          -4.7365, -10.1734, -10.8294, -10.6906,  -7.2625,  -7.0725,  -8.9757,\n",
       "         -10.9926,  -9.4819, -10.4914, -13.5297,  -8.1981,  -9.0000, -10.9145,\n",
       "         -11.0993,  -8.9135,  -9.9709,  -9.1752,  -9.4638,  -8.5327,  -5.1731,\n",
       "         -11.0672,  -2.8805,  -6.9828, -10.3836,  -9.6701, -13.0401,  -6.8854,\n",
       "         -11.5970,  -9.9771, -11.2350, -10.1079,  -5.4348, -11.7316, -10.9294,\n",
       "         -11.0577, -12.5109,  -9.1506,  -4.9626,  -8.1008, -11.9424,  -5.7510,\n",
       "         -11.4179, -11.9201, -10.3690,  -9.7411, -10.6591, -11.3677,  -9.7242,\n",
       "          -8.1207,  -8.5697,  -9.9705, -13.8730, -11.1496, -11.7036, -11.7751,\n",
       "         -12.1139,  -7.1244,  -9.3080, -10.7958, -10.3847, -11.4370,  -9.5531,\n",
       "          -8.0776, -12.1460, -10.5871,  -7.2401, -15.4866,  -5.2221, -12.2255,\n",
       "         -14.0981, -12.6300, -11.1963, -11.2098,  -7.1938,  -8.8046,  -9.7160,\n",
       "         -11.5903,  -9.4140, -10.0889, -11.0382, -10.7021,  -8.4921, -12.2446,\n",
       "         -11.2213,  -7.0183,  -6.6564,  -9.2579, -12.6312, -10.8357,  -4.7532,\n",
       "          -9.4111,  -7.7582, -10.3548, -12.7100,  -5.8012,  -6.1002, -10.4738,\n",
       "          -3.3503,  -4.0794, -11.1150, -11.2891,  -9.5014, -10.3223,  -9.4925,\n",
       "         -11.8982, -12.5952, -10.3122,  -9.7089,  -9.4483,  -6.0174, -10.5460,\n",
       "          -4.6270, -11.7459, -12.6573, -11.0344, -14.1302,  -9.3120, -13.2813,\n",
       "          -8.8117,  -7.1720,  -9.8016, -11.4126, -11.9748,  -6.3389, -12.0972,\n",
       "          -7.3395, -12.2960,  -4.1378,  -7.9751, -14.4322, -11.3783, -12.1973,\n",
       "          -9.6492,  -7.2536, -11.2687,  -8.5321,  -8.1825,  -8.9633,  -7.7109,\n",
       "         -10.4124,  -7.3493, -11.3378, -13.9780, -14.0481, -11.0110,  -8.9481,\n",
       "          -5.2842, -16.0857, -11.7126,  -9.1796,  -4.1074, -11.6222, -11.2133,\n",
       "          -9.1792, -10.3787, -10.2995, -12.3377, -13.2663, -14.2948,  -8.8363,\n",
       "         -15.6952,  -8.1207,  -8.4509, -13.7208, -11.5814,  -6.6795, -12.9447,\n",
       "         -13.0023, -10.3796, -11.3469, -12.0293, -10.8470, -11.4082, -10.8480,\n",
       "          -9.6198,  -8.8325,  -7.0938,  -9.3053,  -6.3274,  -5.2115, -12.1111,\n",
       "         -11.9535, -11.4306, -10.0200,  -6.6795, -12.6393,  -5.2115,  -7.6571,\n",
       "         -14.4763,  -9.5957,  -6.6839, -10.4284,  -8.4216, -10.2543,  -9.8725,\n",
       "         -11.3216,  -9.8502,  -9.4994, -13.0482, -12.0342,  -4.1074,  -3.1239,\n",
       "         -11.4886,  -5.4523,  -5.4587, -11.9425,  -7.8584, -12.5318, -12.8948,\n",
       "          -8.2619, -10.0321,  -9.0246,  -6.8775, -12.2647, -10.6478,  -7.5044,\n",
       "          -9.8645,  -4.7628,  -7.7164,  -9.3442, -11.0378, -10.9114,  -8.1661,\n",
       "          -8.5077,  -6.8432, -13.3177, -10.0811,  -9.0620, -11.4305, -15.0457,\n",
       "         -13.4472, -13.0628,  -9.3949, -11.4508,  -7.3745,  -9.9487, -12.6788,\n",
       "         -11.4966,  -9.9726, -10.1913,  -7.2232, -14.7606, -12.6427,  -9.6140,\n",
       "         -13.3549,  -4.1717,  -7.6151, -10.5639, -11.1006, -11.8329, -10.9235,\n",
       "          -9.2168, -10.2808,  -6.4566, -11.0523, -11.0225, -11.7814, -10.6300,\n",
       "          -5.7510, -11.6866, -12.3929, -10.7726, -12.5303, -12.8274,  -9.0564,\n",
       "         -10.9942, -12.2071, -12.0633, -10.3053, -12.2636,  -4.2082,  -4.1378,\n",
       "          -6.1064, -11.3779, -10.3118, -14.2616, -10.3636,  -8.0580, -12.2908,\n",
       "          -4.0978,  -9.9019, -11.4688, -11.5453, -12.4471, -10.8514,  -8.5637,\n",
       "          -7.4590, -10.8447, -11.4227, -12.3049, -12.7164, -11.8273,  -9.4717,\n",
       "          -8.7561,  -4.7399, -10.8386,  -9.8103, -14.4553,  -6.8213,  -9.2359,\n",
       "         -11.5921,  -7.9549,  -5.6042,  -9.0862,  -8.9420, -11.9146,  -9.6982,\n",
       "         -10.3433,  -5.2221, -12.1431, -10.1132,  -3.8073,  -7.3618,  -5.7716,\n",
       "         -11.0404,  -9.0326,  -8.6402, -10.8448,  -5.7510,  -7.2050,  -8.1120,\n",
       "         -10.5417, -13.3806,  -4.6595, -10.3514,  -7.6929,  -3.8621,  -7.5143,\n",
       "         -10.4207, -10.4832,  -4.7475, -10.4040, -10.0889, -11.5879,  -9.4447,\n",
       "          -6.4913, -13.3283,  -7.5257, -10.8145, -12.3052, -11.8556, -11.1741,\n",
       "         -12.1902, -12.9545,  -9.7275, -11.1817, -10.3373,  -3.8073, -10.7710,\n",
       "          -8.3540,  -8.8309, -10.7021,  -9.8499]),\n",
       " 'full_model_pred_label': tensor(38)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75cb3ccc-2214-4a9d-9c5f-a5e190f47676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2806, 23194, 28975, 11328, 45560,  1717, 26070, 20522, 49398, 36026,\n",
       "        46578, 37777, 33859,  8078, 36421, 29713,  3175, 11294, 37079,  2547,\n",
       "        48471, 14457,  6866, 46154, 27902, 29679, 24579,   347,  3944, 28293,\n",
       "         7517,  9470, 12313,  9589, 29141, 44665, 22372, 14074, 33605,  7049,\n",
       "        16849,  7979, 24017, 13316, 43676,   911,  9643,   367, 17321, 35247,\n",
       "        14576,  1810,   911, 21574,  1902, 35794,   309, 42222, 13612,   309,\n",
       "        22676,  2447, 41782, 47561,   347, 27437, 37277, 26482, 24485, 16639,\n",
       "        10593, 12568, 40644, 49251, 25567, 21105, 41578, 42998, 36839,  1041,\n",
       "        20818,  9502, 12088, 38681, 33368,   943,  2869,   509, 34438, 32955,\n",
       "        26914,  5506, 17456,  9910, 24533, 44373, 24320,   337, 49931, 46997,\n",
       "        20741, 30311, 36980, 12164, 34802, 36126, 41388, 41624, 14679,   347,\n",
       "        23995, 14021, 21435,  6182, 48823, 31890, 29702, 41946,   347, 34974,\n",
       "        46216, 14538,  2066, 42748, 16952, 18008, 49268,  5215, 44985, 23732,\n",
       "        19375, 25028, 21891, 37090, 32473,  2258, 34780,  3794, 37198, 19101,\n",
       "        46820, 10843, 27820, 43231, 20353, 31057, 17073,  1623, 42922, 50008,\n",
       "        16940, 40946, 43956,  4849, 38496, 27229,  1879,  8124,  1081, 23825,\n",
       "        29293, 32211, 18580,  9309, 16291,  9530, 30872, 37921,  4650, 17389,\n",
       "        24269, 13244, 38252, 30596, 17113, 32982, 30928,  9827, 18714, 31625,\n",
       "         7057, 14119,   350,  7576, 11582, 22410, 10674,   327, 36443, 11243,\n",
       "        31561,  8774, 31592, 27026,   371, 29066,  8857, 12517, 11790, 31630,\n",
       "          399, 34753, 49860, 43663, 36865, 28549, 11853, 21028, 10656, 32490,\n",
       "        36353,  1052, 15336,  9801, 19562,  4068, 34037, 25051, 28589,  3738,\n",
       "        49383, 20529,   350,  1001, 48500, 25040, 39476,   679,  3337, 21204,\n",
       "        21003,   402, 47318, 31988, 10500,  1004,  3908, 16519, 16629, 15142,\n",
       "        29913, 22559,  5844, 27885, 16490, 21165, 12492, 13458, 13308,  6586,\n",
       "         5828, 39847, 37313, 15196, 10529, 19398, 20757, 24153, 19902, 49404,\n",
       "        14939, 10026, 25656,   376, 49657, 11520,  8919, 26943, 14909,  7859,\n",
       "        18593, 19743,   520,  1475, 32825, 15375,  5335, 15796,  7802,  1012,\n",
       "         5027, 12926, 22917, 34520, 43084, 33609, 34612, 47538, 41427, 24520,\n",
       "        46991, 14819, 22828,  6612, 18286, 42555, 14167, 12873, 50075,   406,\n",
       "        31104, 43792, 47717, 11287, 35206,  4849,  7595,  5326, 14778, 12300,\n",
       "        21532, 31864, 13643,  6342, 47782, 13708,  1514,  1962, 31721, 35293,\n",
       "         7055, 20882,  4842,  3687,  5648, 14493,  2692, 25299, 15238,  1168,\n",
       "          337, 12871, 23434, 12406,  3423,  3936, 31074, 45138,  7312, 37445,\n",
       "         1951, 28760,  5398,  3284, 29285, 21418,  8942, 33208, 35773, 39927,\n",
       "        23356, 40753, 17837, 46993, 13340,  6455, 20400,  3738, 23731,   311,\n",
       "         4347, 47177, 33972, 19362,  3442, 28028, 40274,  4881,  9544, 16445,\n",
       "         2688, 10433, 40959, 32183, 46896, 44317, 28537,  8602,   383, 43586,\n",
       "        10553, 19483,   367, 46694, 36498, 47688,  8031, 15664, 27139, 28319,\n",
       "        37118,  3831, 33899,  5027, 46369, 34732,  9329,  4392, 45919, 39540,\n",
       "        35007, 35193, 29599, 14734, 23918, 44418, 44615, 14708,  3175, 14546,\n",
       "         4100,   978, 12671, 45301, 22777, 12173,   554, 22221,   978, 42619,\n",
       "        29224, 17871,  1709, 20552, 27028, 48968, 28440, 11769, 22418,  4744,\n",
       "        10140, 18456,   367,   347, 10202,  7137, 34474, 41898, 22489, 35332,\n",
       "        32108, 46070,  7492,   775, 22761, 21450, 10306,  5060, 18460,   350,\n",
       "        11618, 11144, 11852, 17857,  4486, 30912,  4312, 33617, 24377, 16666,\n",
       "        34910, 46811, 35597, 38633, 30693, 26249,  4689, 14159, 10727, 19250,\n",
       "        38507,  5638, 16581, 40868,  9406, 27872, 48269, 32369,  3208, 31393,\n",
       "        33328, 43296, 33452, 13512, 47678,  3932, 23592, 26345, 30597, 10278,\n",
       "          520, 23519, 46106, 20071, 30155, 30422, 47660, 36805, 28442, 29018,\n",
       "        19610, 14973,  3944,   311,  1703, 36980,  9533, 36852, 10710, 10315,\n",
       "        34649,   309, 29242, 29575, 38579, 18918, 37270,  7648,  3576,  6206,\n",
       "        10769, 46117, 26608, 15555,  7011,  9371,  9070, 38739, 12551, 35925,\n",
       "        19761, 40702, 25768, 24388,  2580, 18687, 31899, 40695, 16256,  8437,\n",
       "          406, 36298, 16640,   509,  6866,  2409, 10836, 37562,  6365,  9072,\n",
       "          520, 11307,  3340, 23475, 34564, 16385,  8838, 10598,   327, 23670,\n",
       "        18220, 27874,   575,  4505, 19800, 38830,  3469,  4434, 33098,  8037,\n",
       "         4492, 18260, 13854, 39636, 49301, 25794, 29638, 21506, 13465,   509,\n",
       "        31832, 31295, 17322, 13643, 28036])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "capital_vocab_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0d6d42-5023-4239-8f51-c86c38020806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/leiyu/projects/def-yangxu/leiyu/LMs/'\n",
    "model_name = 'gpt2-small'\n",
    "\n",
    "model_path = join(model_dir, model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1874d1b-8782-4d47-ae0a-3bac31c0db77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠBro',\n",
       " 'ĠJacksonville',\n",
       " 'ĠCroatia',\n",
       " 'ĠWarren',\n",
       " 'ĠBohem',\n",
       " 'ĠEd',\n",
       " 'ĠCharleston',\n",
       " 'ĠMemphis',\n",
       " 'ĠWarwick',\n",
       " 'ĠToledo',\n",
       " 'ĠDover',\n",
       " 'ĠYugoslavia',\n",
       " 'ĠCatalonia',\n",
       " 'ĠJordan',\n",
       " 'ĠUruguay',\n",
       " 'ĠBelfast',\n",
       " 'ĠPal',\n",
       " 'ĠCas',\n",
       " 'ĠPrague',\n",
       " 'ĠPar',\n",
       " 'ĠEaton',\n",
       " 'ĠCambridge',\n",
       " 'ĠSar',\n",
       " 'ĠKarachi',\n",
       " 'ĠBulgaria',\n",
       " 'ĠStockholm',\n",
       " 'ĠAuburn',\n",
       " 'ĠB',\n",
       " 'ĠBel',\n",
       " 'ĠManitoba',\n",
       " 'ĠIreland',\n",
       " 'ĠKansas',\n",
       " 'ĠVictoria',\n",
       " 'ĠPennsylvania',\n",
       " 'ĠSyracuse',\n",
       " 'ĠConstantine',\n",
       " 'ĠSeoul',\n",
       " 'ĠOttawa',\n",
       " 'ĠAzerbaijan',\n",
       " 'ĠUkraine',\n",
       " 'ĠLebanon',\n",
       " 'ĠBon',\n",
       " 'ĠVic',\n",
       " 'ĠPhilippines',\n",
       " 'ĠFiji',\n",
       " 'ĠSh',\n",
       " 'ĠPhoenix',\n",
       " 'ĠH',\n",
       " 'ĠNewton',\n",
       " 'ĠMecca',\n",
       " 'ĠPhilip',\n",
       " 'ĠWar',\n",
       " 'ĠSh',\n",
       " 'ĠMilan',\n",
       " 'ĠGr',\n",
       " 'ĠLatvia',\n",
       " 'ĠT',\n",
       " 'ĠOral',\n",
       " 'ĠParker',\n",
       " 'ĠT',\n",
       " 'ĠTall',\n",
       " 'ĠAug',\n",
       " 'ĠOman',\n",
       " 'ĠStras',\n",
       " 'ĠB',\n",
       " 'ĠBoulder',\n",
       " 'ĠIrvine',\n",
       " 'ĠSalvador',\n",
       " 'ĠHassan',\n",
       " 'ĠCzech',\n",
       " 'ĠBarb',\n",
       " 'ĠBeat',\n",
       " 'ĠHuntington',\n",
       " 'ĠJakarta',\n",
       " 'ĠCork',\n",
       " 'ĠQueensland',\n",
       " 'ĠLima',\n",
       " 'ĠTours',\n",
       " 'ĠSlovakia',\n",
       " 'ĠPro',\n",
       " 'ĠNaval',\n",
       " 'ĠManchester',\n",
       " 'ĠTel',\n",
       " 'ĠStevenson',\n",
       " 'ĠBelarus',\n",
       " 'ĠAr',\n",
       " 'ĠJapan',\n",
       " 'ĠK',\n",
       " 'ĠHalifax',\n",
       " 'ĠWarsaw',\n",
       " 'ĠBrisbane',\n",
       " 'ĠAnn',\n",
       " 'ĠChile',\n",
       " 'ĠSab',\n",
       " 'ĠWyoming',\n",
       " 'ĠFulton',\n",
       " 'ĠSouthwest',\n",
       " 'ĠM',\n",
       " 'ĠJudah',\n",
       " 'ĠSard',\n",
       " 'ĠMartinez',\n",
       " 'ĠGujarat',\n",
       " 'ĠGuang',\n",
       " 'ĠMurray',\n",
       " 'ĠSantiago',\n",
       " 'ĠScarborough',\n",
       " 'ĠLafayette',\n",
       " 'ĠMaharashtra',\n",
       " 'ĠSwitzerland',\n",
       " 'ĠB',\n",
       " 'ĠGlasgow',\n",
       " 'ĠFranklin',\n",
       " 'ĠBurg',\n",
       " 'ĠBoston',\n",
       " 'ĠPortsmouth',\n",
       " 'ĠWebster',\n",
       " 'ĠVenice',\n",
       " 'ĠMedina',\n",
       " 'ĠB',\n",
       " 'ĠWilliamson',\n",
       " 'ĠBangalore',\n",
       " 'ĠArkansas',\n",
       " 'ĠRepublic',\n",
       " 'ĠKyr',\n",
       " 'ĠThailand',\n",
       " 'ĠNan',\n",
       " 'ĠCarth',\n",
       " 'ĠGen',\n",
       " 'ĠAberdeen',\n",
       " 'ĠCairo',\n",
       " 'ĠSacramento',\n",
       " 'ĠHerman',\n",
       " 'ĠAthens',\n",
       " 'ĠBurlington',\n",
       " 'ĠMarion',\n",
       " 'ĠNorth',\n",
       " 'ĠPratt',\n",
       " 'ĠIndia',\n",
       " 'ĠDudley',\n",
       " 'ĠPortugal',\n",
       " 'ĠAugusta',\n",
       " 'ĠJerusalem',\n",
       " 'ĠVernon',\n",
       " 'ĠNormandy',\n",
       " 'ĠWestminster',\n",
       " 'ĠCambodia',\n",
       " 'ĠHob',\n",
       " 'ĠCol',\n",
       " 'ĠHiroshima',\n",
       " 'ĠMonteneg',\n",
       " 'ĠRichmond',\n",
       " 'ĠBots',\n",
       " 'ĠSalmon',\n",
       " 'ĠSal',\n",
       " 'ĠBolivia',\n",
       " 'ĠSerbia',\n",
       " 'ĠCar',\n",
       " 'ĠCarl',\n",
       " 'ĠAs',\n",
       " 'ĠVienna',\n",
       " 'ĠLeeds',\n",
       " 'ĠBrunswick',\n",
       " 'ĠMans',\n",
       " 'ĠColumbia',\n",
       " 'ĠCircle',\n",
       " 'ĠHan',\n",
       " 'ĠUganda',\n",
       " 'ĠDund',\n",
       " 'ĠMany',\n",
       " 'ĠButler',\n",
       " 'ĠTrent',\n",
       " 'ĠNav',\n",
       " 'ĠNicaragua',\n",
       " 'ĠStafford',\n",
       " 'ĠGran',\n",
       " 'ĠHonduras',\n",
       " 'ĠDurham',\n",
       " 'ĠBatman',\n",
       " 'ĠUnity',\n",
       " 'ĠKarn',\n",
       " 'ĠArm',\n",
       " 'ĠTaiwan',\n",
       " 'ĠP',\n",
       " 'ĠLim',\n",
       " 'ĠHamilton',\n",
       " 'ĠNewcastle',\n",
       " 'ĠElizabeth',\n",
       " 'ĠC',\n",
       " 'ĠTulsa',\n",
       " 'ĠPa',\n",
       " 'ĠWindsor',\n",
       " 'ĠMain',\n",
       " 'ĠEthiopia',\n",
       " 'ĠNepal',\n",
       " 'ĠR',\n",
       " 'ĠBahrain',\n",
       " 'ĠPhiladelphia',\n",
       " 'ĠDelhi',\n",
       " 'ĠTokyo',\n",
       " 'ĠMonaco',\n",
       " 'ĠN',\n",
       " 'ĠKabul',\n",
       " 'ĠLansing',\n",
       " 'ĠSamoa',\n",
       " 'ĠPend',\n",
       " 'ĠSteele',\n",
       " 'ĠGot',\n",
       " 'ĠDamascus',\n",
       " 'ĠDenver',\n",
       " 'ĠTaj',\n",
       " 'ĠMacedonia',\n",
       " 'ĠAn',\n",
       " 'ĠMalaysia',\n",
       " 'ĠBang',\n",
       " 'ĠHungary',\n",
       " 'ĠIran',\n",
       " 'ĠVil',\n",
       " 'ĠHaiti',\n",
       " 'ĠZimbabwe',\n",
       " 'ĠAnt',\n",
       " 'ĠGlou',\n",
       " 'ĠDh',\n",
       " 'ĠP',\n",
       " 'ĠSe',\n",
       " 'ĠAden',\n",
       " 'ĠBristol',\n",
       " 'ĠCherokee',\n",
       " 'ĠHe',\n",
       " 'ĠCenter',\n",
       " 'ĠPierre',\n",
       " 'ĠLag',\n",
       " 'ĠG',\n",
       " 'ĠTripoli',\n",
       " 'ĠSalem',\n",
       " 'ĠStock',\n",
       " 'ĠLe',\n",
       " 'ĠIraq',\n",
       " 'ĠArgentina',\n",
       " 'ĠMilwaukee',\n",
       " 'ĠBarcelona',\n",
       " 'ĠPerth',\n",
       " 'ĠStuart',\n",
       " 'ĠCre',\n",
       " 'ĠCyprus',\n",
       " 'ĠDenmark',\n",
       " 'ĠEdmonton',\n",
       " 'ĠYemen',\n",
       " 'ĠManhattan',\n",
       " 'ĠRaj',\n",
       " 'ĠToronto',\n",
       " 'ĠMexico',\n",
       " 'ĠTir',\n",
       " 'ĠBav',\n",
       " 'ĠNag',\n",
       " 'ĠOd',\n",
       " 'ĠNigeria',\n",
       " 'ĠYun',\n",
       " 'ĠLakes',\n",
       " 'ĠKil',\n",
       " 'ĠNaples',\n",
       " 'ĠColumbus',\n",
       " 'ĠHarris',\n",
       " 'ĠRhodes',\n",
       " 'ĠF',\n",
       " 'ĠSenegal',\n",
       " 'ĠGraham',\n",
       " 'ĠMinnesota',\n",
       " 'ĠChester',\n",
       " 'ĠMadison',\n",
       " 'ĠGeorgia',\n",
       " 'ĠAleppo',\n",
       " 'ĠSaga',\n",
       " 'ĠSt',\n",
       " 'ĠEx',\n",
       " 'ĠGhana',\n",
       " 'ĠJefferson',\n",
       " 'ĠMary',\n",
       " 'ĠDouglas',\n",
       " 'ĠDavis',\n",
       " 'ĠCl',\n",
       " 'ĠGal',\n",
       " 'ĠAlaska',\n",
       " 'ĠMumbai',\n",
       " 'ĠRaleigh',\n",
       " 'ĠCamden',\n",
       " 'ĠLund',\n",
       " 'ĠKingston',\n",
       " 'ĠBuckingham',\n",
       " 'ĠSof',\n",
       " 'ĠDubai',\n",
       " 'ĠKaufman',\n",
       " 'ĠMelbourne',\n",
       " 'ĠMao',\n",
       " 'ĠJackson',\n",
       " 'ĠQatar',\n",
       " 'ĠPrescott',\n",
       " 'ĠStanley',\n",
       " 'ĠPoland',\n",
       " 'ĠWorcester',\n",
       " 'ĠL',\n",
       " 'ĠCopenhagen',\n",
       " 'ĠHutchinson',\n",
       " 'ĠLivingston',\n",
       " 'ĠTennessee',\n",
       " 'ĠMalta',\n",
       " 'ĠSal',\n",
       " 'ĠBrazil',\n",
       " 'ĠWood',\n",
       " 'ĠQuebec',\n",
       " 'ĠGab',\n",
       " 'ĠMontgomery',\n",
       " 'ĠLuxembourg',\n",
       " 'ĠOxford',\n",
       " 'ĠParis',\n",
       " 'ĠJericho',\n",
       " 'ĠHawaii',\n",
       " 'ĠGo',\n",
       " 'ĠGu',\n",
       " 'ĠManila',\n",
       " 'ĠBolton',\n",
       " 'ĠMichigan',\n",
       " 'ĠTehran',\n",
       " 'ĠChicago',\n",
       " 'ĠEast',\n",
       " 'ĠDun',\n",
       " 'ĠConnecticut',\n",
       " 'ĠIsrael',\n",
       " 'ĠIstanbul',\n",
       " 'ĠNorway',\n",
       " 'ĠZ',\n",
       " 'ĠM',\n",
       " 'ĠMontreal',\n",
       " 'ĠWinnipeg',\n",
       " 'ĠLincoln',\n",
       " 'ĠHere',\n",
       " 'ĠTexas',\n",
       " 'ĠPearson',\n",
       " 'ĠYok',\n",
       " 'ĠSeattle',\n",
       " 'ĠKosovo',\n",
       " 'ĠChrist',\n",
       " 'ĠAnkara',\n",
       " 'ĠCanadian',\n",
       " 'ĠRussia',\n",
       " 'ĠAlbany',\n",
       " 'ĠArnold',\n",
       " 'ĠBurn',\n",
       " 'ĠLiberia',\n",
       " 'ĠValencia',\n",
       " 'ĠFresno',\n",
       " 'ĠRomania',\n",
       " 'ĠLub',\n",
       " 'ĠFinland',\n",
       " 'ĠDresden',\n",
       " 'ĠLouisiana',\n",
       " 'ĠBas',\n",
       " 'ĠGibson',\n",
       " 'ĠAnt',\n",
       " 'ĠNiger',\n",
       " 'ĠS',\n",
       " 'ĠPort',\n",
       " 'ĠIps',\n",
       " 'ĠTunisia',\n",
       " 'ĠIndianapolis',\n",
       " 'ĠCalifornia',\n",
       " 'ĠBabylon',\n",
       " 'ĠHavana',\n",
       " 'ĠFrance',\n",
       " 'ĠHo',\n",
       " 'ĠBrussels',\n",
       " 'ĠWest',\n",
       " 'ĠOklahoma',\n",
       " 'ĠBudapest',\n",
       " 'ĠGuatemala',\n",
       " 'ĠTasmania',\n",
       " 'ĠBoone',\n",
       " 'ĠFlorence',\n",
       " 'ĠSpain',\n",
       " 'ĠThe',\n",
       " 'ĠNorwich',\n",
       " 'ĠOntario',\n",
       " 'ĠBangladesh',\n",
       " 'ĠH',\n",
       " 'ĠMadagascar',\n",
       " 'ĠAlgeria',\n",
       " 'ĠHelsinki',\n",
       " 'ĠItaly',\n",
       " 'ĠBelgium',\n",
       " 'ĠAdelaide',\n",
       " 'ĠProvidence',\n",
       " 'ĠHartford',\n",
       " 'ĠTur',\n",
       " 'ĠGap',\n",
       " 'ĠGal',\n",
       " 'ĠConstantinople',\n",
       " 'ĠConcord',\n",
       " 'ĠLind',\n",
       " 'ĠSyria',\n",
       " 'ĠLaos',\n",
       " 'ĠLexington',\n",
       " 'ĠBangkok',\n",
       " 'ĠLyon',\n",
       " 'ĠMali',\n",
       " 'ĠLiberty',\n",
       " 'ĠSomalia',\n",
       " 'ĠZamb',\n",
       " 'ĠAlbania',\n",
       " 'ĠMadrid',\n",
       " 'ĠPal',\n",
       " 'ĠLuck',\n",
       " 'ĠMac',\n",
       " 'ĠAl',\n",
       " 'ĠNetherlands',\n",
       " 'ĠBihar',\n",
       " 'ĠGuinea',\n",
       " 'ĠMobile',\n",
       " 'ĠIn',\n",
       " 'ĠLogan',\n",
       " 'ĠAl',\n",
       " 'ĠUzbek',\n",
       " 'ĠLeicester',\n",
       " 'ĠNam',\n",
       " 'ĠBr',\n",
       " 'ĠGeneva',\n",
       " 'ĠKuwait',\n",
       " 'ĠAngola',\n",
       " 'ĠHomer',\n",
       " 'ĠWales',\n",
       " 'ĠMunich',\n",
       " 'ĠFlorida',\n",
       " 'ĠMassachusetts',\n",
       " 'ĠEagle',\n",
       " 'ĠH',\n",
       " 'ĠB',\n",
       " 'ĠUtah',\n",
       " 'ĠTurkey',\n",
       " 'ĠKazakhstan',\n",
       " 'ĠLisbon',\n",
       " 'ĠMonte',\n",
       " 'ĠCrowley',\n",
       " 'ĠClayton',\n",
       " 'ĠBrune',\n",
       " 'ĠColorado',\n",
       " 'ĠWe',\n",
       " 'ĠKiev',\n",
       " 'ĠDarwin',\n",
       " 'ĠCleveland',\n",
       " 'ĠSum',\n",
       " 'ĠNice',\n",
       " 'ĠP',\n",
       " 'ĠBeijing',\n",
       " 'ĠTai',\n",
       " 'ĠSydney',\n",
       " 'ĠBryan',\n",
       " 'ĠGermany',\n",
       " 'ĠEstonia',\n",
       " 'ĠBer',\n",
       " 'ĠJerome',\n",
       " 'ĠAmsterdam',\n",
       " 'ĠVenezuela',\n",
       " 'ĠCardiff',\n",
       " 'ĠTownsend',\n",
       " 'ĠRiverside',\n",
       " 'ĠAlbuquerque',\n",
       " 'ĠLah',\n",
       " 'ĠMonroe',\n",
       " 'ĠLa',\n",
       " 'ĠCuba',\n",
       " 'ĠPortland',\n",
       " 'ĠNashville',\n",
       " 'ĠRwanda',\n",
       " 'ĠWater',\n",
       " 'ĠBh',\n",
       " 'ĠWinchester',\n",
       " 'ĠIowa',\n",
       " 'ĠAlexandria',\n",
       " 'ĠTrinidad',\n",
       " 'ĠArmenia',\n",
       " 'ĠPat',\n",
       " 'ĠGeorgetown',\n",
       " 'ĠPunjab',\n",
       " 'ĠHonolulu',\n",
       " 'ĠCanberra',\n",
       " 'ĠBrand',\n",
       " 'ĠChennai',\n",
       " 'ĠBen',\n",
       " 'ĠBurns',\n",
       " 'ĠPont',\n",
       " 'ĠWellington',\n",
       " 'ĠIndiana',\n",
       " 'ĠSt',\n",
       " 'ĠPanama',\n",
       " 'ĠMaurit',\n",
       " 'ĠIdaho',\n",
       " 'ĠGreenland',\n",
       " 'ĠAuckland',\n",
       " 'ĠHyder',\n",
       " 'ĠTucson',\n",
       " 'ĠGlad',\n",
       " 'ĠKens',\n",
       " 'ĠSudan',\n",
       " 'ĠEnterprise',\n",
       " 'ĠBel',\n",
       " 'ĠS',\n",
       " 'ĠAm',\n",
       " 'ĠGuang',\n",
       " 'ĠAustin',\n",
       " 'ĠHampton',\n",
       " 'ĠSweden',\n",
       " 'ĠGreece',\n",
       " 'ĠKerala',\n",
       " 'ĠT',\n",
       " 'ĠSax',\n",
       " 'ĠMiranda',\n",
       " 'ĠJohannes',\n",
       " 'ĠPalmer',\n",
       " 'ĠTanzania',\n",
       " 'ĠPakistan',\n",
       " 'ĠLondon',\n",
       " 'ĠBern',\n",
       " 'ĠMaryland',\n",
       " 'ĠPapua',\n",
       " 'ĠConway',\n",
       " 'ĠAlberta',\n",
       " 'ĠFair',\n",
       " 'ĠAtlanta',\n",
       " 'ĠMoscow',\n",
       " 'ĠRegina',\n",
       " 'ĠSingapore',\n",
       " 'ĠParsons',\n",
       " 'ĠBaghdad',\n",
       " 'ĠBeirut',\n",
       " 'ĠPeru',\n",
       " 'ĠLv',\n",
       " 'ĠChe',\n",
       " 'ĠNord',\n",
       " 'ĠAin',\n",
       " 'ĠSavannah',\n",
       " 'ĠIndonesia',\n",
       " 'ĠMiami',\n",
       " 'ĠL',\n",
       " 'ĠKyoto',\n",
       " 'ĠMack',\n",
       " 'ĠK',\n",
       " 'ĠSar',\n",
       " 'ĠBar',\n",
       " 'ĠVietnam',\n",
       " 'ĠSlovenia',\n",
       " 'ĠEgypt',\n",
       " 'ĠCentre',\n",
       " 'ĠSt',\n",
       " 'ĠBerlin',\n",
       " 'ĠCanada',\n",
       " 'ĠEdinburgh',\n",
       " 'ĠPreston',\n",
       " 'ĠKaz',\n",
       " 'ĠScotland',\n",
       " 'ĠRome',\n",
       " 'ĠC',\n",
       " 'ĠBuch',\n",
       " 'ĠDublin',\n",
       " 'ĠSpringfield',\n",
       " 'ĠY',\n",
       " 'ĠAustralia',\n",
       " 'ĠChad',\n",
       " 'ĠBoise',\n",
       " 'ĠGreen',\n",
       " 'ĠMal',\n",
       " 'ĠCrosby',\n",
       " 'ĠAfghanistan',\n",
       " 'ĠEngland',\n",
       " 'ĠGlobe',\n",
       " 'ĠCraig',\n",
       " 'ĠOslo',\n",
       " 'ĠSicily',\n",
       " 'ĠEcuador',\n",
       " 'ĠMorocco',\n",
       " 'ĠKenya',\n",
       " 'ĠLibya',\n",
       " 'ĠK',\n",
       " 'ĠVale',\n",
       " 'ĠLithuania',\n",
       " 'ĠAustria',\n",
       " 'ĠOxford',\n",
       " 'ĠMoz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(capital_vocab_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ae7bf-8278-4804-ab60-870789b890fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a2f8d1-9df9-4947-a738-ecdcad458d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data_dir = '/home/leiyu/projects/def-yangxu/leiyu/circuit-discovery/data/'\n",
    "model_dir = '/home/leiyu/projects/def-yangxu/leiyu/LMs/'\n",
    "model_name = 'gpt2-small'\n",
    "\n",
    "with open(join(data_dir, 'pararel_data_all.json')) as open_file:\n",
    "    pararel_rel_data = json.load(open_file)  \n",
    "\n",
    "model_path = join(model_dir, model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e331a326-189a-47a2-8852-e7fd99d04af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m             answer_vocab\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m target)\n\u001b[1;32m     21\u001b[0m answer_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(ds_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer_vocab\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids)\n\u001b[1;32m     23\u001b[0m answer_vocab_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m     24\u001b[0m     input_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m tokenizer(answer_vocab)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     25\u001b[0m ])\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m         )\n\u001b[1;32m   2957\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2997\u001b[0m     )\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3149\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3140\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3141\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3142\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3147\u001b[0m )\n\u001b[0;32m-> 3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:162\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:537\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    536\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_and_encodings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    538\u001b[0m     stack \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m item, _ \u001b[38;5;129;01min\u001b[39;00m tokens_and_encodings \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m item[key]]\n\u001b[1;32m    539\u001b[0m     sanitized_tokens[key] \u001b[38;5;241m=\u001b[39m stack\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rel_id = 'P136'\n",
    "\n",
    "data = pararel_rel_data[rel_id]\n",
    "    \n",
    "answer_vocab = []\n",
    "ds_dict = {\n",
    "    'prompt': [],\n",
    "    'answer': [],\n",
    "}\n",
    "\n",
    "for entry in data:\n",
    "    prompt = entry[0][0].replace(' [MASK] .', '')\n",
    "    prompt = prompt.replace(' [MASK].', '')\n",
    "    if '[MASK]' not in prompt:\n",
    "        target = entry[0][1]\n",
    "        if target:\n",
    "            ds_dict['prompt'].append(prompt)\n",
    "            ds_dict['answer'].append(' ' + target)\n",
    "            answer_vocab.append(' ' + target)\n",
    "\n",
    "answer_vocab = list(set(ds_dict['answer']))\n",
    "print(tokenizer(answer_vocab).input_ids)\n",
    "answer_vocab_idx = torch.tensor([\n",
    "    input_ids[0] for input_ids in tokenizer(answer_vocab).input_ids\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a090da4e-0c4b-442f-a223-38a283f156d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27852be-7297-43b3-8710-44f0a3dc3bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
