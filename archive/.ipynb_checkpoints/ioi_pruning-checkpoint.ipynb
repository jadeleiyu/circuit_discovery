{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15d4008-818a-41d3-ac44-f38fd558a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pickle\n",
    "import transformer_lens\n",
    "from torch.optim import AdamW\n",
    "from os.path import join\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from masked_model import MaskedModel\n",
    "from circuit_gpt import CircuitGPT, CircuitGPTConfig\n",
    "from utils import get_target_module_keys\n",
    "from ioi_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624edade-3b38-44ee-bda1-58b7647591a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae3e964-a3ba-46a1-b7be-4c2ff42096e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diff_mask_ioi.yml') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(join(args['model_dir'], args['model_name']))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = IOIDataset(prompt_type=\"ABBA\", N=640, tokenizer=tokenizer)\n",
    "ds_train, ds_test = train_test_split(ds.ioi_prompts, test_size=0.2, random_state=0)\n",
    "# Note that there are overlaps between train and test sets, due to the way IOIDataset is constructed (randomly sample N items)\n",
    "\n",
    "ioi_ds_train = CircuitIOIDataset(prepare_ioi_data_for_clm(ds_train))\n",
    "ioi_ds_test = CircuitIOIDataset(prepare_ioi_data_for_clm(ds_test))\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    ioi_ds_train,\n",
    "    batch_size=args['batch_size']\n",
    ")\n",
    "eval_dl = DataLoader(\n",
    "    ioi_ds_train,\n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55235e1b-7120-4e1f-bb80-47f9f3ff4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_faith_loss(batch_logits, batch_inputs):\n",
    "    # batch_logits: (B, seq_len, vocab_size)\n",
    "    batch_seq_lens = batch_inputs['seq_lens']\n",
    "    batch_size = batch_logits.shape[0]\n",
    "\n",
    "    logits_target_good = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target good']]\n",
    "    logits_target_bad = batch_logits[torch.arange(batch_size), batch_seq_lens - 1, batch_inputs['target bad']]\n",
    "    logits_gb = torch.stack([logits_target_good, logits_target_bad], -1)  # (B,2)\n",
    "\n",
    "    batch_labels = torch.zeros(batch_size).long().to(logits_gb.device)\n",
    "    batch_faith_loss = nn.functional.cross_entropy(logits_gb, batch_labels)\n",
    "\n",
    "    return batch_faith_loss, logits_gb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_mask_model(masked_model, eval_dl, tokenizer, device, use_mask=True):\n",
    "    masked_model.model.eval()\n",
    "    masked_model.apply_masks(deterministic_masks=True)  # weight masks\n",
    "    masked_model.model.turn_on_edge_masks(deterministic_masks=True)  # edge masks\n",
    "\n",
    "    _, _, pruned_model_density_weight = masked_model.get_pruned_model_density()\n",
    "    _, _, pruned_model_density_edge = masked_model.model.get_pruned_model_density()\n",
    "\n",
    "    if not use_mask:\n",
    "        masked_model.remove_masks()\n",
    "        masked_model.model.turn_off_edge_masks()\n",
    "\n",
    "    total = len(eval_dl.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "        batch_logits = masked_model(batch_inputs['input_ids'].to(device))[0]  # (B, seq_len, vocab_size)\n",
    "        _, batch_logits_gb = compute_faith_loss(batch_logits, batch_inputs)\n",
    "        correct += (batch_logits_gb[:, 0] > batch_logits_gb[:, 1]).sum()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if use_mask:\n",
    "        masked_model.remove_masks()\n",
    "        masked_model.model.turn_off_edge_masks()\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    return acc.item(), pruned_model_density_weight, pruned_model_density_edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d7f6c8-c1c1-4077-a067-bdea41425d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# # download gpt2-small weights from EasyTransformer and save it\n",
    "# reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "# torch.save(reference_gpt2.state_dict(), join(args['model_dir'], 'gpt2-small/gpt2_small_weights.pt'))\n",
    "\n",
    "gpt_weights = torch.load(join(args['model_dir'], 'gpt2-small/gpt2_small_weights.pt')) \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(join(args['model_dir'], args['model_name']))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "circuit_gpt_config = CircuitGPTConfig(\n",
    "    debug=False,\n",
    "    gs_temp=args['temperature_edge'],\n",
    "    mask_logit_init=args['mask_logit_init']\n",
    ")\n",
    "circuit_gpt = CircuitGPT(circuit_gpt_config, means=None)\n",
    "circuit_gpt.load_state_dict(gpt_weights, strict=False)\n",
    "circuit_gpt.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d29352-1a86-42eb-bb0b-83e6d5d58f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['param_keys'] = get_target_module_keys(circuit_gpt)\n",
    "masked_model = MaskedModel(circuit_gpt, args)\n",
    "\n",
    "# weight_mask_fn = join(args['model_dir'], 'mask_logits', 'gpt2_small_ioi_weight_mask_logits.pt')\n",
    "# masked_model.load_mask_logits(weight_mask_fn, device)\n",
    "\n",
    "# optim_weight = torch.optim.AdamW(masked_model.get_trainable_parameters(), lr=args['lr_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d9c859e-084b-40cf-bc79-810dd9f186af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# eval_acc, pruned_model_density_weight, _ = eval_mask_model(masked_model, eval_dl, tokenizer, device, use_mask=False)\n",
    "# print(\"Epoch 0. unmasked model accuracy {:.2f}\".format(eval_acc))\n",
    "eval_acc, pruned_model_density_weight, pruned_model_density_edge = eval_mask_model(masked_model, eval_dl, tokenizer, device, use_mask=False)\n",
    "print(\n",
    "    \"Epoch 0. mean pruned model accuracy: {:.2f}, weight density: {:.4f}, edge density: {:.4f}\".format(\n",
    "        eval_acc, pruned_model_density_weight, pruned_model_density_edge)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291007f-9e91-40f6-b567-8123a552eb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e8034a-c666-4bbf-b4ea-36fa3dc1c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lambda_sparse_edge(epoch):\n",
    "#     if epoch < 20:\n",
    "#         return 0.01 * epoch / 20\n",
    "#     elif epoch < 100:\n",
    "#         return 0.01 - 0.009 * epoch / 100\n",
    "#     else:\n",
    "#         return 0.001\n",
    "\n",
    "\n",
    "# def get_lambda_sparse_weight(epoch, args):\n",
    "#     if epoch <= 20:\n",
    "#         return args['lambda_sparse_weight']\n",
    "#     elif epoch < 1020:\n",
    "#         return args['lambda_sparse_weight'] + epoch - 20\n",
    "#     else:\n",
    "#         return 1000\n",
    "\n",
    "\n",
    "# def weight_pruning_epoch(args, epoch, train_dl, eval_dl, masked_model, optim_weight):\n",
    "#     # masked_model.model.turn_off_edge_masks()\n",
    "\n",
    "#     lambda_sparse_w = get_lambda_sparse_weight(epoch, args)\n",
    "\n",
    "#     for batch in train_dl:\n",
    "#         batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "\n",
    "#         # weight pruning\n",
    "#         masked_model.apply_masks()\n",
    "#         batch_logits = masked_model(batch_inputs['input_ids'].to(device))[0] # (B, seq_len, vocab_size)\n",
    "#         faith_loss_weight, _ = compute_faith_loss(batch_logits, batch_inputs)\n",
    "#         # losses['faithfulness'].append(faith_loss_weight.detach().cpu().item())\n",
    "#         sparse_loss_weight = masked_model.get_sparseness_loss()\n",
    "#         # losses['sparseness'].append(sparse_loss_weight.detach().cpu().item())\n",
    "#         loss_weight = sparse_loss_weight * lambda_sparse_w + faith_loss_weight\n",
    "#         loss_weight.backward()\n",
    "#         optim_weight.step()\n",
    "#         optim_weight.zero_grad()\n",
    "#         masked_model.remove_masks()\n",
    "#         # torch.cuda.empty_cache()\n",
    "\n",
    "#     eval_acc, pruned_model_density_weight, pruned_model_density_edge = eval_mask_model(masked_model, eval_dl, tokenizer, device)\n",
    "#     print(\n",
    "#         \"Epoch {}. mean pruned model accuracy {:.2f}, weight density: {:.4f}\".format(\n",
    "#             epoch + 1, eval_acc, pruned_model_density_weight))\n",
    "#     # print('\\n')\n",
    "\n",
    "\n",
    "# def edge_pruning_epoch(args, epoch, train_dl, eval_dl, masked_model, circuit_gpt, optim_edge):\n",
    "\n",
    "#     circuit_gpt.turn_on_edge_masks(deterministic_masks=False)\n",
    "#     masked_model.apply_masks(deterministic_masks=True)\n",
    "\n",
    "#     for batch in train_dl:\n",
    "#         batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "\n",
    "#         lambda_sparse_edge = get_lambda_sparse_edge(epoch)\n",
    "#         circuit_gpt.train() \n",
    "#         circuit_gpt.turn_on_edge_masks(deterministic_masks=False)\n",
    "#         sparse_loss_edge = 0\n",
    "#         for p in circuit_gpt.mask_params_edge:\n",
    "#             sparse_loss_edge += nn.functional.sigmoid(p).sum()\n",
    "    \n",
    "#         batch_logits = circuit_gpt(batch_inputs['input_ids'].to(device))[0] \n",
    "#         faith_loss_edge, _ = compute_faith_loss(batch_logits, batch_inputs)        \n",
    "#         loss_edge = faith_loss_edge + sparse_loss_edge *  lambda_sparse_edge\n",
    "#         loss_edge.backward()\n",
    "#         optim_edge.step()\n",
    "#         optim_edge.zero_grad()\n",
    "#         # torch.cuda.empty_cache()\n",
    "\n",
    "#     eval_acc, pruned_model_density_weight, pruned_model_density_edge = eval_mask_model(masked_model, eval_dl, tokenizer, device)\n",
    "#     print(\n",
    "#         \"Epoch {}. mean pruned model accuracy: {:.2f}, weight density: {:.4f}, edge density: {:.4f}\".format(\n",
    "#             epoch + 1, eval_acc, pruned_model_density_weight, pruned_model_density_edge))\n",
    "\n",
    "\n",
    "# def combined_pruning(args, masked_model, circuit_gpt, train_dl, eval_dl, n_epoch_edge=100, n_epoch_weight_per_edge_epoch=10):\n",
    "#     optim_weight = torch.optim.AdamW(masked_model.get_trainable_parameters(), lr=args['lr_weight'])\n",
    "#     optim_edge = torch.optim.AdamW(circuit_gpt.mask_params_edge, lr=args['lr_edge'])\n",
    "\n",
    "#     epoch = 0\n",
    "#     for i in tqdm(range(n_epoch_edge), position=1):\n",
    "#         for j in tqdm(range(n_epoch_weight_per_edge_epoch), position=0):\n",
    "#             weight_pruning_epoch(args, epoch, train_dl, eval_dl, masked_model, optim_weight)\n",
    "#             epoch += 1\n",
    "#         edge_pruning_epoch(args, epoch, train_dl, eval_dl, masked_model, circuit_gpt, optim_edge)\n",
    "#         epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5383d421-ec67-4548-8f9a-aced38516e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combined_pruning(\n",
    "#     args, masked_model, circuit_gpt, train_dl, eval_dl, n_epoch_edge=100, n_epoch_weight_per_edge_epoch=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347bef3-3847-4e1a-b3fd-4e17004d9034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f9b55c-0790-4bf5-ad66-e5020293ed35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weight pruning\n",
    "# torch.cuda.empty_cache()\n",
    "# masked_model.model.turn_off_edge_masks()\n",
    "\n",
    "\n",
    "# def get_lambda_sparse_weight_ioi(epoch, args):\n",
    "#     if epoch <= 20:\n",
    "#         return args['lambda_sparse_weight']\n",
    "#     else:\n",
    "#         return args['lambda_sparse_weight'] + (epoch - 40)* 1.0\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(args['train_epochs_weight'])):\n",
    "\n",
    "#     # if epoch > 20 and args['lambda_sparse_weight'] < 1000:\n",
    "#     #     args['lambda_sparse_weight'] += 1\n",
    "#     lambda_sparse_w = get_lambda_sparse_weight_ioi(epoch, args)\n",
    "\n",
    "#     for batch in train_dl:\n",
    "#         batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "\n",
    "#         # weight pruning\n",
    "#         masked_model.apply_masks()\n",
    "#         batch_logits = masked_model(batch_inputs['input_ids'].to(device))[0] # (B, seq_len, vocab_size)\n",
    "        \n",
    "#         faith_loss_weight, _ = compute_faith_loss(batch_logits, batch_inputs)\n",
    "\n",
    "#         sparse_loss_weight = masked_model.get_sparseness_loss()\n",
    "#         loss_weight = sparse_loss_weight * lambda_sparse_w + faith_loss_weight\n",
    "#         loss_weight.backward()\n",
    "#         optim_weight.step()\n",
    "#         optim_weight.zero_grad()\n",
    "#         masked_model.remove_masks()\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     eval_acc, pruned_model_density_weight, pruned_model_density_edge = eval_mask_model(masked_model, eval_dl, tokenizer, device)\n",
    "#     print(\n",
    "#         \"Epoch {}. mean pruned model accuracy {:.2f}, weight density: {:.4f}\".format(\n",
    "#             epoch + 1, eval_acc, pruned_model_density_weight))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c65c536-f3fb-4347-8041-2ca182b2acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weight masks\n",
    "# weight_mask_fn = join(args['model_dir'], 'mask_logits', 'gpt2_small_ioi_weight_mask_logits.pt')\n",
    "# masked_model.save_mask_logits(weight_mask_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbf20e8-4442-46a2-84dd-e6e7648ee77e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:05<49:43,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:12<51:50,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:18<52:18,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [00:25<52:27,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:31<52:51,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [00:38<52:37,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [00:44<52:55,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/500 [00:50<52:34,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [00:57<52:36,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9. mean pruned model accuracy: 1.00, weight density: 1.0000, edge density: 1.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [00:57<52:43,  6.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m circuit_gpt\u001b[38;5;241m.\u001b[39mmask_params_edge:\n\u001b[1;32m     37\u001b[0m     sparse_loss_edge \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msigmoid(p)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 39\u001b[0m batch_logits \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m     40\u001b[0m faith_loss_edge, _ \u001b[38;5;241m=\u001b[39m compute_faith_loss(batch_logits, batch_inputs)        \n\u001b[1;32m     41\u001b[0m loss_edge \u001b[38;5;241m=\u001b[39m faith_loss_edge \u001b[38;5;241m+\u001b[39m sparse_loss_edge \u001b[38;5;241m*\u001b[39m  lambda_sparse_edge\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre07/scratch/leiyu/lm_circuit/circuit_gpt.py:326\u001b[0m, in \u001b[0;36mCircuitGPT.forward\u001b[0;34m(self, tokens, return_states)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(residual, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch position d_model -> batch position 1 d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# if hasattr(self,\"saved_states\"):\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m#     self.saved_states = torch.cat((self.saved_states, block.saved_output.unsqueeze(0)), dim=0)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m#     self.saved_states = block.saved_output.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_states:\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre07/scratch/leiyu/lm_circuit/circuit_gpt.py:224\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, means)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# self.saved_output = attn_out\u001b[39;00m\n\u001b[1;32m    221\u001b[0m residual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((resid_pre, attn_out), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m masked_mlp_residual \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch position prev_head_idx d_model, prev_head_idx -> batch position d_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_edge_mask_mlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(masked_mlp_residual)\n\u001b[1;32m    227\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(normalized_resid_mid)\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(operands[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[38;5;241m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_equation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, equation, \u001b[38;5;241m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# edge pruning\n",
    "\n",
    "def get_lambda_sparse_edge(epoch, lambda_0=1e-3, lambda_1=2e-3, lambda_2=1e-4, n_epoch_warmup=20, n_epoch_cooldown=30):\n",
    "    if epoch < n_epoch_warmup:\n",
    "        return lambda_0 + (lambda_1 - lambda_0) * (epoch / n_epoch_warmup)    \n",
    "    elif epoch < n_epoch_warmup + n_epoch_cooldown:\n",
    "        return lambda_1 - (lambda_1 - lambda_2) * (epoch - n_epoch_warmup) / n_epoch_cooldown\n",
    "    else:\n",
    "        return lambda_2\n",
    "    \n",
    "optim_edge = AdamW(circuit_gpt.mask_params_edge, lr=args['lr_edge'])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "circuit_gpt.turn_on_edge_masks(deterministic_masks=False)\n",
    "masked_model.apply_masks(deterministic_masks=True)\n",
    "\n",
    "for epoch in tqdm(range(args['train_epochs_edge'])):\n",
    "\n",
    "    lambda_sparse_edge = get_lambda_sparse_edge(\n",
    "        epoch, \n",
    "        lambda_0=args['lambda_sparse_edge'], \n",
    "        lambda_1=2*args['lambda_sparse_edge'],\n",
    "        lambda_2=0.1*args['lambda_sparse_edge'],\n",
    "        n_epoch_warmup=50,\n",
    "        n_epoch_cooldown=450\n",
    "    )\n",
    "    lambda_sparse_edge = args['lambda_sparse_edge']\n",
    "\n",
    "    for batch in train_dl:\n",
    "        batch_inputs = prepare_batch_inputs(batch, tokenizer)\n",
    "\n",
    "        # lambda_sparse_edge = args['lambda_sparse_edge']\n",
    "        circuit_gpt.train() \n",
    "        circuit_gpt.turn_on_edge_masks(deterministic_masks=False)\n",
    "        sparse_loss_edge = 0\n",
    "        for p in circuit_gpt.mask_params_edge:\n",
    "            sparse_loss_edge += nn.functional.sigmoid(p).sum()\n",
    "    \n",
    "        batch_logits = circuit_gpt(batch_inputs['input_ids'].to(device))[0] \n",
    "        faith_loss_edge, _ = compute_faith_loss(batch_logits, batch_inputs)        \n",
    "        loss_edge = faith_loss_edge + sparse_loss_edge *  lambda_sparse_edge\n",
    "        loss_edge.backward()\n",
    "        optim_edge.step()\n",
    "        optim_edge.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    eval_acc, pruned_model_density_weight, pruned_model_density_edge = eval_mask_model(masked_model, eval_dl, tokenizer, device)\n",
    "    print(\n",
    "        \"Epoch {}. mean pruned model accuracy: {:.2f}, weight density: {:.4f}, edge density: {:.4f}\".format(\n",
    "            epoch + 1, eval_acc, pruned_model_density_weight, pruned_model_density_edge))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036b202-8954-4075-86be-a37b1d80dd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1c913-b261-4382-936a-01adbd236f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2256b-cee3-4f2f-8b7d-1b5eb621ee7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd75a1-c00b-49cf-9804-909900995bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce16f8-5320-40d0-8e3e-c099b8760d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5685439-e6a0-4551-a87e-a3ca331ba977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d6886-e02f-4ed0-ab63-f32b41d64517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
